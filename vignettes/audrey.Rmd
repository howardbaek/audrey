---
title: "Introduction to audrey"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to audrey}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

This vignette introduces you to audrey's functions for automatic speech recognition (ASR) with OpenAI's [Whisper](https://openai.com/research/whisper). Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web. When the OpenAI developers measured Whisperâ€™s zero-shot performance across many diverse datasets,  they found it is much more robust and makes 50% fewer errors than other models.

To utilize this system, you must download Whisper and ffmpeg on your system. Refer to Whisper's [Setup](https://github.com/openai/whisper#setup) documentation for more information.

## Speech Recognition

```{r setup}
library(audrey)
```

`transcribe()` performs speech recognition by converting spoken language from the input audio file into written text in its original language. 

For example, let's say we provide the following audio file as input:

<audio controls>
    <source src="audio/sample1.mp3" type="audio/mp3"/>
</audio>

```{r}
transcribe("audio/sample1.mp3", output_dir = ".")
```

If the function successfully ran, you will receive a success message and the location where the output txt file has been stored. To obtain the transcribed text generated by speech recognition, 

```{r}
sample1_text <- read_
```

